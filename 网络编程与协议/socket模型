基于socket通信的并发连接设计，有多种模型设计
1. 根据《Unix网络编程》中的描述，预分配多线程模式的进程控制时间开销最少，而在各种锁中，mutex方式比文件锁的效率要高
2. 并发程序设计中，过多子进程/子线程的问题 ———— 惊群问题
3. 用一个子进程/线程池来服务客户时，客户请求在池内的分布问题
4. Leader/Followers模式的问题：假如所有的服务线程都已经accept且正在处理请求，那么新的client发起connect时，虽然会被listen中的队列所接收，却无法被accept并处理。所以此时新client会发现请求的处理依旧有延迟。
p642

一、 迭代模型
		1. 单进程单线程accept，无进程调度消耗，但无法并发
二、 并发模型
		2. 多进程服务器
			
		3. 多线程服务器

		4. (延迟)预分配多进程服务器：
			1）Leader/Followers模式，无锁accept
					多个子进程accept睡眠在同一个socket描述符上，当一个连接到来时，此socket的成员so_timeo变化会唤醒所有睡眠的子进程。（只有第一个被调度的进程获取连接）
					这就是所谓的“惊群”问题。因为会唤醒所有的进程，所以会导致不必要的性能下降。子进程数越多，下降越多。
					不过某些unix内核有wakeup_one函数，它只唤醒一个子进程，不会引起惊群。
			2）Leader/Followers模式，文件锁accept
			3）Leader/Followers模式，线程mutex锁accept
			4）half-sync/half-async模式的accept。即由父进程向子进程传递socket描述符
		5. (延迟)预分配多线程服务器：
			1）Leader/Followers模式，用mutex保护accept
				同样有一个队列，不过不需要模式实现者显示构造，而是直接使用了操作系统底层的队列。 
				例子：在 《unix网络编程》 一书的 “27.11 TCP 预先创建服务器线程，每个线程各自 accept ” 的例子
					int listenfd;  
					
					int main( int argc, char * argv[] )  
					{  
						......  
						listenfd = Tcp_listen( NULL, argv[ 1 ], &addrlen );  
						......  
						for( int i = 0; i < nthreads; i++ ){  
							pthread_create( &tptr[i].thread_tid, NULL, &thread_main, (void*)i );  
					 	}  
					 ......  
				 	}  
						 
					void * thread_main( void * arg )  
					{  
						for( ; ; ){  
							 ......  
							 [b]// 多个线程同时阻塞在这个 accept 调用上，依靠操作系统的队列[/b]  
							 connfd = accept( listenfd, cliaddr, &clilen );  
							 ......  
							 web_child( connfd );  
							 close( connfd );  
							 ......  
						}  
					}  			
			2）half-sync/half-async模式的accept。由主线程调用accept，将获取的socket描述符传给子线程
				由模式实现者显示构造一个队列，以便同步层和异步层可以通信。半同步/半异步设计由程序员自己构造队列，而领导者/追随者设计使用OS内核中的队列。在《POSA2》中，作者的评价：因为半同步/半异步设计在 web 服务器虚拟内存而不是操作系统内核内排队请求，所以它更具伸缩性。 
				例子：在 《unix网络编程》 一书的 “27.12 TCP预先创建线程服务器程序，主线程统一 accept” 的例子
					[b]//这就是一个典型的循环队列的定义，iget 是队列头，iput 是队列尾[/b]  
					int clifd[MAXNCLI], iget, iput;   
						
					int main( int argc, char * argv[] )  
					{  
						......  
						int listenfd = Tcp_listen( NULL, argv[ 1 ], &addrlen );  
						......  
						
					 iget = iput = 0;  
					 
					 for( int i = 0; i < nthreads; i++ ) {  
						 pthread_create( &tptr[i].thread_tid, NULL, &thread_main, (void*)i );  
					 
					 for( ; ; ) {  
						 connfd = accept( listenfd, cliaddr,, &clilen );  
						 clifd[ iput ] = connfd;     [b]// 接受到的连接句柄放入队列[/b]  
						 if( ++iput == MAXNCLI ) iput = 0;    
					 }  
				 }  
					 
				 void * thread_main( void * arg )  
				 {  
					 for( ; ; ) {  
						 while( iget == iput ) pthread_cond_wait( ...... );  
						 connfd = clifd[ iget ];     [b]// 从队列中获得连接句柄[/b]  
						 if( ++iget == MAXNCLI ) iget = 0;  
						 ......  
						 web_child( connfd );  
						 close( connfd );  
					 }  
				 }  
三、 轮询模型
		6. select/poll服务器
				select是一个或多个进程同时监听多个fd。而只要有一个fd变化，则唤醒所有阻塞在此fd上的进程，可能会导致select的冲突问题。
				1）因select增加了一个系统调用，且需要处理冲突，所以阻塞在select上不如直接阻塞在accept上。
				2）防止拒绝服务攻击：服务器决不能阻塞于单个client相关的调用，这样会因一个恶意的用户而导致拒绝对其它合法用户提供服务。解决的办法就是用非阻塞的select，或多进程/线程
				3）最后一个time_out参数表示select在所有fd都没有事件时循环等待的时间。
					
		7. pselect
				select的posix标准，增加了一个信号量参数。
				
		8. poll：提供了与select相似的功能，但在用于流设备上时还会提供额外的信息。
		
四、 事件通知模型
		9. epoll调用
				原理：epoll相当于一个file，file的private_data域中维护了一个红黑树（eventpoll），此树为注册到epoll set的所有fd生成一个结点，这个结点包含fd指针以及相应的回调函数。
				eventpoll有三个等待队列：wait，poll_wait以及rdlist （ready的fd list）
				当epoll的wait队列返回时，相应有事件的epitem对应的回调函数ep_poll_callback就会将此结点的fd加入到readlist中；如果是LT方式，那么在从readlist中拿出来处理后还要插入到队列尾部？？？

				（参见man epoll）
				1）epoll_create():创建一个epoll实例，返回epoll文件描述符(fd)；
				2）epoll_ctl()：将需要监听的fd加入到epoll的set中；
				3）epoll_wait()；等待set中某个fd的epoll event发生；
					epoll有两种触发方式：EPOLLET 和 Default(LT方式)，如果是EPOLLET方式，则每次epoll_wait()返回后必须一次性读完发送方发来的数据，才能再调用epoll_wait()，否则调用epoll_wait()会block。所以，EPOLLET应该以如下方式使用：
						a）使用non-blocking 文件描述符
						b）只在read(2)或者 write(2) 返回EAGAIN的情况下才调用epoll_wait()。
				4）LT方式触发时，它就相当于一个更快的poll，所有poll能用的地方，都能用LT方式的epoll来代替，因为此时两者有相同的语义。
				5) ET方式触发时，调用者必须负责使用EPOLL_CTL_MOD参数来调用epoll_ctl(2)，并且可以使用EPOLLONESHOT来让epoll_wait(2)对某一fd的事件不做反应


				/proc/sys/fs/epoll/max_user_watches 接口的限定：
					它指定了每一个USER ID在所有的epoll实例上能注册的fd总数：每个fd在32位系统中占大约90个字节。max_user_watches的默认值就是(4%) of the available low memory除以90的值。

						8. IOCP模型
							1）Reactor：基于同步IO的事件通知机制
							2）Proactor：基于异步IO的事件通知机制











